{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":72489,"databundleVersionId":8096274,"sourceType":"competition"},{"sourceId":8220915,"sourceType":"datasetVersion","datasetId":4873674}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    üî•üî•ABALONE AGE (CODE)üî•üî•\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://http2.mlstatic.com/concha-de-abulon-en-bruto-por-kilo-D_NQ_NP_3729-MLM4623475852_072013-F.jpg)","metadata":{}},{"cell_type":"markdown","source":"Here is some detailed information about Abalone Age, organized in bullet points and sub-bullet points:\n\n- **What are Abalones**:\n  - Abalones are a type of marine mollusk that belongs to the Haliotidae family.\n  - They are commonly found in coastal waters and are known for their colorful shells and edible meat.\n\n- **Life Cycle**:\n  - Abalones have a relatively long life cycle, with some species living up to 30 years or more.\n  - They start their life as larvae, which hatch from eggs laid by adult abalones.\n  - Larvae drift in the ocean for several weeks before settling on suitable substrate to grow into juveniles.\n  - Juvenile abalones gradually develop their characteristic shell and body structure as they mature.\n\n- **Determining Age**:\n  - Age determination in abalones can be challenging but is often done through various methods:\n    - **Shell Rings**: Similar to tree rings, abalone shells can show growth rings that indicate age.\n    - **Shell Length and Growth Rate**: Measuring shell length and growth rate can provide estimates of age.\n    - **Chemical Analysis**: Isotopic or chemical analysis of shell material can also provide insights into age.\n  \n- **Factors Affecting Growth and Age**:\n  - Several factors can influence the growth rate and age of abalones:\n    - **Environmental Conditions**: Water temperature, food availability, and habitat quality play crucial roles.\n    - **Genetics**: Different abalone species and populations may have varying growth rates and maximum ages.\n    - **Predation and Competition**: Predation pressure and competition for resources can affect growth and survival.\n  \n- **Importance in Fisheries**:\n  - Abalones are commercially valuable and are harvested for their meat, which is considered a delicacy in many cuisines.\n  - Overfishing and habitat degradation have led to concerns about the sustainability of abalone fisheries.\n  - Age determination studies help in understanding population dynamics and setting sustainable harvest quotas.\n\n- **Conservation Efforts**:\n  - Conservation efforts for abalones often focus on:\n    - **Fisheries Management**: Implementing quotas, size limits, and protected areas to prevent overexploitation.\n    - **Habitat Protection**: Preserving coastal habitats and improving water quality to support healthy abalone populations.\n    - **Research and Monitoring**: Continued research on abalone biology, population trends, and threats to inform conservation strategies.\n\n- **Cultural and Ecological Significance**:\n  - Abalones hold cultural significance in many indigenous communities, where they are used in ceremonies and traditional practices.\n  - Ecologically, abalones are important as grazers that help maintain the health of kelp forests and rocky shore ecosystems.\n\nUnderstanding abalone age and its significance is crucial for sustainable management and conservation of these valuable marine species.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    üìúIMPORTING LIBRARIESüìú\n</div>\n","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np \nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor,VotingRegressor,HistGradientBoostingRegressor\nfrom sklearn.metrics import accuracy_score,r2_score,recall_score,roc_auc_score,confusion_matrix,classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nimport warnings\nimport optuna\nimport keras_tuner\nimport kerastuner\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.exceptions import DataConversionWarning\nfrom catboost import CatBoostRegressor,Pool\nfrom catboost.utils import eval_metric\nwarnings.filterwarnings(\"ignore\", category=DataConversionWarning)\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import display, HTML\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import cross_val_score\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom scipy.stats import ttest_ind,chi2_contingency\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.utils import class_weight\n\nimport pandas as pd \nfrom sklearn.preprocessing import PowerTransformer, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom scipy.stats import skew\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n!pip install ipywidgets -q\n!pip install \"flaml[automl]\" -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin: 20px; padding: 20px; border-radius: 10px; border: 2px solid #4CAF50; background-color: #E6F7E2;\">\n    <b>üìå Dataset Path:</b> Successfully fetch the Dataset Path\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin: 20px; padding: 20px; border-radius: 10px; border: 2px solid #4CAF50; background-color: #E6F7E2;\">\n    <b>üìÇ Libraries:</b> Successfully import the recquired library\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    SETTING FOR üîçVISUALISATION AND BACKGROUND\n</div>\n","metadata":{}},{"cell_type":"code","source":"rc = {\n    \"axes.facecolor\": \"#F8F8F8\",\n    \"figure.facecolor\": \"#F8F8F8\",\n    \"axes.edgecolor\": \"#000000\",\n    \"grid.color\": \"#EBEBE7\" + \"30\",\n    \"font.family\": \"serif\",\n    \"axes.labelcolor\": \"#000000\",\n    \"xtick.color\": \"#000000\",\n    \"ytick.color\": \"#000000\",\n    \"grid.alpha\": 0.4,\n}\n\nsns.set(rc=rc)\npalette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n\nfrom colorama import Style, Fore\nblk = Style.BRIGHT + Fore.BLACK\nmgt = Style.BRIGHT + Fore.MAGENTA\nred = Style.BRIGHT + Fore.RED\nblu = Style.BRIGHT + Fore.BLUE\nres = Style.RESET_ALL\n\nplt.style.use('fivethirtyeight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    READING THE TRAIN AND TEST FILE PATH\n</div>\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s4e4/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s4e4/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_nn = pd.read_csv(\"/kaggle/input/playground-series-s4e4/train.csv\")\ntest_nn = pd.read_csv(\"/kaggle/input/playground-series-s4e4/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin: 20px; padding: 20px; border-radius: 10px; border: 2px solid #4CAF50; background-color: #E6F7E2;\">\n    <b>üìÇ DataSet:</b> Successfully read the dataset \n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    TRAIN AND TEST DATASET SAMPLE\n</div>\n","metadata":{}},{"cell_type":"code","source":"train.sample(5).style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.sample(5).style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    INFORMATION ABOUT TRAIN AND TEST DATA \n</div>\n","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin: 20px; padding: 20px; border-radius: 10px; border: 2px solid #4CAF50; background-color: skyblue;\">\n<b>üîç Information:</b> \n    \n1. 10 columns in train data.\n    \n    \n2. Many of them are int data type, many are float datatype, and many are object datatype\n</div>","metadata":{}},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin: 20px; padding: 20px; border-radius: 10px; border: 2px solid #4CAF50; background-color: skyblue;\">\n<b>üîç Information:</b> \n    \n1. 9 columns in test data.\n    \n    \n2. Many of them are int data type, many are float datatype, and many are object datatype\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n     STATICAL INFORMATION ABOUT TRAIN AND TEST DATA \n</div>\n","metadata":{}},{"cell_type":"code","source":"train.describe().T.style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe().T.style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    NULL VALUE CHECK IN TRAIN AND TEST DATA  \n</div>\n","metadata":{}},{"cell_type":"code","source":"sns.displot(data=train.isnull().melt(value_name = 'missing'),\n           y = 'variable',\n           hue = 'missing',multiple='fill',height=8,aspect = 1.6)\nplt.axvline(0.4,color = 'r')\nplt.title(\"Null values in train data\",fontsize = 13)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data=test.isnull().melt(value_name = 'missing'),\n           y = 'variable',\n           hue = 'missing',multiple='fill',height=8,aspect = 1.6)\nplt.axvline(0.4,color = 'r')\nplt.title(\"Null values in train data\",fontsize = 13)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin: 20px; padding: 20px; border-radius: 10px; border: 2px solid #4CAF50; background-color: skyblue;\">\n<b>üîç Information:</b> No null value present in train and test data\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    üí´CHECKING FOR DUPLICATEüí´ \n</div>\n","metadata":{}},{"cell_type":"code","source":"train.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    TARGET COLUMN ANALYSIS  \n</div>\n","metadata":{}},{"cell_type":"code","source":"# Plotting\nplt.figure(figsize=(14, 6))\n\n# Plotting histogram and KDE for 'Rings' column\nplt.subplot(1, 2, 1)\nplt.hist(train['Rings'], bins=30, color='skyblue', edgecolor='black')\nplt.title('Distribution of Rings', fontsize=14, fontweight='bold')\nplt.xlabel('Rings', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(axis='y', linestyle='--', alpha=0.5)\nplt.tight_layout()\n\n# Plotting KDE only for 'Rings' column\nplt.subplot(1, 2, 2)\nsns.kdeplot(train['Rings'], color='black')\nplt.title('Kernel Density Estimation (KDE) for Rings', fontsize=14, fontweight='bold')\nplt.xlabel('Rings', fontsize=12)\nplt.ylabel('Density', fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(axis='y', linestyle='--', alpha=0.5)\nplt.tight_layout()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    TRANING DATA AND TESTING DATA ANALYSIS\n</div>\n","metadata":{}},{"cell_type":"code","source":"unique_counts = train.nunique()\n#Threshold to distinguish continous and categorical\nthreshold = 12\ncontinuous_vars = unique_counts[unique_counts > threshold].index.tolist()\ncategorical_vars = unique_counts[unique_counts <= threshold].index.tolist()\nif 'id' in continuous_vars:\n    continuous_vars.remove('id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    üéáCATEGORICAL COLUMN ANALYSISüéá\n</div>\n","metadata":{}},{"cell_type":"code","source":"custom_palette = sns.color_palette(\"pastel\")\n\nfor column in categorical_vars:\n    f, ax = plt.subplots(1, 2, figsize=(18, 5.5))\n    train[column].value_counts().plot.pie(autopct='%1.1f%%', ax=ax[0], shadow=True, colors=custom_palette)\n    ax[0].set_ylabel(f'{column}')\n    sns.countplot(x=column, data=train, ax=ax[1], palette=custom_palette)\n    ax[0].set_title(f'{column} Distribution (Pie Chart)', fontsize=14)\n    ax[1].set_title(f'{column} Count (Bar Plot)', fontsize=14)\n    plt.suptitle(f'{column} Visualization', fontsize=16, fontweight='bold')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    ‚ö°CONTINUOUS COLUMN ANALYSIS‚ö° \n</div>\n","metadata":{}},{"cell_type":"code","source":"# Set up warnings to be ignored (optional)\nwarnings.filterwarnings(\"ignore\")\npd.set_option('mode.use_inf_as_na', False)\n\n# List of continuous variables in your dataset\ncontinuous_vars = ['Length', 'Diameter', 'Height', 'Whole weight', 'Whole weight.1', 'Whole weight.2', 'Shell weight']\n\n# Set hue to your target column\ntarget_column = 'Rings'\n\nfor column in continuous_vars:\n    fig, axes = plt.subplots(1, 2, figsize=(18, 4))  # Create subplots with 1 row and 2 columns\n    \n    # Plot histogram with hue\n    sns.histplot(data=train, x=column, hue=target_column, bins=50, kde=True, ax=axes[0], palette='muted')\n    axes[0].set_title(f'Histogram of {column} with {target_column} Hue')\n    axes[0].set_xlabel(column)\n    axes[0].set_ylabel('Count')\n    axes[0].legend(title=target_column, loc='upper right')\n    \n    # Plot KDE plot with hue\n    sns.kdeplot(data=train, x=column, hue=target_column, ax=axes[1], palette='muted')\n    axes[1].set_title(f'KDE Plot of {column} with {target_column} Hue')\n    axes[1].set_xlabel(column)\n    axes[1].set_ylabel('Density')\n    axes[1].legend(title=target_column, loc='upper right')\n    \n    plt.tight_layout()  # Adjust spacing between subplots\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    ‚ú®ANALYSIS BY QQ PLOT‚ú®\n</div>\n","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats  \ndef qq_plot_with_skewness(data, quantitative_var):\n    # Check if the variable is present in the DataFrame\n    if quantitative_var not in data.columns:\n        print(f\"Error: '{quantitative_var}' not found in the DataFrame.\")\n        return\n    \n    f, ax = plt.subplots(1, 2, figsize=(18, 5.5))\n\n    # Check for missing values\n    if data[quantitative_var].isnull().any():\n        print(f\"Warning: '{quantitative_var}' contains missing values. Results may be affected.\")\n\n    # QQ plot\n    stats.probplot(data[quantitative_var], plot=ax[0], fit=True)\n    ax[0].set_title(f'QQ Plot for {quantitative_var}')\n\n    # Skewness plot\n    sns.histplot(data[quantitative_var], kde=True, ax=ax[1])\n    ax[1].set_title(f'Distribution of {quantitative_var}')\n\n    # Calculate skewness value\n    skewness_value = stats.skew(data[quantitative_var])\n\n    # Display skewness value on the plot\n    ax[1].text(0.5, 0.5, f'Skewness: {skewness_value:.2f}', transform=ax[1].transAxes, \n               horizontalalignment='center', verticalalignment='center', fontsize=16, color='red')\n\n    plt.show()\n# Example usage for each continuous variable\nfor var in continuous_vars:\n    qq_plot_with_skewness(train, var)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    üîçFEATURE ENGENEERINGüîç\n</div>\n","metadata":{}},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Diameter_to_Height_Ratio'] = train['Diameter'] / train['Height']\ntrain['Combined_Whole_Weight'] = train['Whole weight'] + train['Whole weight.1'] + train['Whole weight.2']\ntrain['Diameter_Length_Product'] = train['Diameter'] * train['Length']\nsex_mapping = {'M': 0, 'F': 1, 'I': 2}  \ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntrain['Shell_Volume'] = (4/3) * 3.14 * (train['Diameter'] / 2)**2 * train['Height']\ntrain['Shell_Surface_Area'] = 4 * 3.14 * (train['Diameter'] / 2)**2\ntrain['Shell_Density'] = train['Shell weight'] / train['Shell_Volume']\ntrain['Shell_Thickness'] = train['Height'] - train['Diameter']\ntrain['Shell_Shape_Index'] = train['Shell_Surface_Area'] / train['Shell_Volume']\n#train['Age_to_Size_Ratio'] = train['Age'] / train['Length']\n#train['Distance_from_Shore_Scaled'] = train['Distance_from_Shore'] / train['Distance_from_Shore'].max()\n#train['Seasonality_Indicator'] = train['Collection_Date'].dt.month.apply(lambda x: 'Summer' if x in [6, 7, 8] else 'Winter')\n#train['Shell_Damage_Score'] = train['Shell_Condition'].map({'Good': 1, 'Fair': 0.5, 'Poor': 0})\ntrain['Length_to_Height_Ratio'] = train['Length'] / train['Height']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Diameter_to_Height_Ratio'] = test['Diameter'] / test['Height']\ntest['Combined_Whole_Weight'] = test['Whole weight'] + test['Whole weight.1'] + test['Whole weight.2']\ntest['Diameter_Length_Product'] = test['Diameter'] * test['Length']\nsex_mapping = {'M': 0, 'F': 1, 'I': 2}  \ntest['Sex'] = test['Sex'].map(sex_mapping)\ntest['Shell_Volume'] = (4/3) * 3.14 * (test['Diameter'] / 2)**2 * test['Height']\ntest['Shell_Surface_Area'] = 4 * 3.14 * (test['Diameter'] / 2)**2\ntest['Shell_Density'] = test['Shell weight'] / test['Shell_Volume']\ntest['Shell_Thickness'] = test['Height'] - test['Diameter']\ntest['Shell_Shape_Index'] = test['Shell_Surface_Area'] / test['Shell_Volume']\ntest['Length_to_Height_Ratio'] = test['Length'] / test['Height']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"sex_mapping = {'True': 1, 'False': 0}  \ntrain['Sex_I'] = train['Sex_I'].map(sex_mapping)\ntrain['Sex_M'] = train['Sex_M'].map(sex_mapping)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"sex_mapping = {'True': 1, 'False': 0}  \ntest['Sex_I'] = test['Sex_I'].map(sex_mapping)\ntest['Sex_M'] = test['Sex_M'].map(sex_mapping)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sample(5).style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.sample(5).style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_counts = train.nunique()\n#Threshold to distinguish continous and categorical\nthreshold = 12\ncontinuous_vars_temp = unique_counts[unique_counts > threshold].index.tolist()\n#categorical_vars = unique_counts[unique_counts <= threshold].index.tolist()\nif 'id' in continuous_vars_temp:\n    continuous_vars_temp.remove('id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_columns = train.select_dtypes(include='number').columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    üîçCOLUMN TRANSFORMERüîç\n</div>\n","metadata":{}},{"cell_type":"code","source":"\"\"\"skewness = train.select_dtypes(include=['number']).apply(lambda x: skew(train.dropna()))\ntransformers = []\nskewed_columns = skewness[skewness > 1].index.tolist()\nnon_skewed_columns = skewness[skewness <= 1].index.tolist()\n\nif skewed_columns:\n    transformers.append(('skewed', PowerTransformer(), skewed_columns))\n\nif non_skewed_columns:\n    transformers.append(('non_skewed', StandardScaler(), non_skewed_columns))\n\n# Apply ColumnTransformer\nct = ColumnTransformer(transformers)\ntransformed_data = ct.fit_transform(train)\nsample_data = pd.DataFrame(transformed_data, columns=train.columns)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    üîçOUTLIERS CHECK BY BOX PLOTüîç\n</div>\n","metadata":{}},{"cell_type":"code","source":"def plot_boxplots(data, columns, ncols=2):\n    nrows = (len(columns) + ncols - 1) // ncols\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 4 * nrows))\n\n    for i, column in enumerate(columns):\n        ax = axes[i // ncols, i % ncols] if nrows > 1 else axes[i % ncols]\n\n        if data[column].dtype == 'O':  # 'O' represents object (categorical) dtype\n            sns.countplot(x=column, data=data, ax=ax)\n            ax.set_title(f'Countplot for {column}')\n        else:\n            sns.boxplot(x=column, data=data, ax=ax)\n            ax.set_title(f'Boxplot for {column}')\n\n    plt.tight_layout()\n    plt.show()\n\nplot_boxplots(train, continuous_vars_temp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    üéáREMOVING THE OUTLIERSüéá\n</div>\n","metadata":{}},{"cell_type":"code","source":"def remove_outliers_replace(data, columns, threshold=1.5):\n    data_no_outliers = data.copy()\n\n    for column in columns:\n        Q1 = data_no_outliers[column].quantile(0.25)\n        Q3 = data_no_outliers[column].quantile(0.75)\n        IQR = Q3 - Q1\n\n        lower_bound = Q1 - threshold * IQR\n        upper_bound = Q3 + threshold * IQR\n\n        is_outlier = (data_no_outliers[column] < lower_bound) | (data_no_outliers[column] > upper_bound)\n\n        if data_no_outliers[column].dtype == 'O':  # Categorical column\n            median_value = data_no_outliers.loc[~is_outlier, column].mode().iloc[0]\n            data_no_outliers.loc[is_outlier, column] = median_value\n        else:  # Numerical column\n            mean_value = data_no_outliers.loc[~is_outlier, column].mean()\n            data_no_outliers.loc[is_outlier, column] = mean_value\n\n    return data_no_outliers\n\ncolumns_to_remove_outliers_replace = continuous_vars_temp\ntrain = remove_outliers_replace(train, columns_to_remove_outliers_replace)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_remove_outliers_replace","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_remove_outliers_replace_test = ['Length',\n 'Diameter',\n 'Height',\n 'Whole weight',\n 'Whole weight.1',\n 'Whole weight.2',\n 'Shell weight',\n 'Diameter_to_Height_Ratio',\n 'Combined_Whole_Weight',\n 'Diameter_Length_Product','Shell_Volume',\n 'Shell_Surface_Area',\n 'Shell_Density',\n 'Shell_Thickness',\n 'Shell_Shape_Index',\n 'Length_to_Height_Ratio']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = remove_outliers_replace(test, columns_to_remove_outliers_replace_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_boxplots(data, columns, ncols=2):\n    nrows = (len(columns) + ncols - 1) // ncols\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 4 * nrows))\n\n    for i, column in enumerate(columns):\n        ax = axes[i // ncols, i % ncols] if nrows > 1 else axes[i % ncols]\n\n        if data[column].dtype == 'O':  # 'O' represents object (categorical) dtype\n            sns.countplot(x=column, data=data, ax=ax)\n            ax.set_title(f'Countplot for {column}')\n        else:\n            sns.boxplot(x=column, data=data, ax=ax)\n            ax.set_title(f'Boxplot for {column}')\n\n    plt.tight_layout()\n    plt.show()\n\nplot_boxplots(train, continuous_vars_temp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\" style=\"margin: 20px; padding: 20px; border-radius: 10px; border: 2px solid #4CAF50; background-color: #FFCCCC;\">\n    <b>‚ò† Danger:</b> Before train the model just just convert all the categorical columns \n</div>\n","metadata":{}},{"cell_type":"code","source":"train.drop(columns='id',axis = 1,inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.sample(5).style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    Dividing the independent and dependent columns seperately  \n</div>\n","metadata":{}},{"cell_type":"code","source":"# Assuming your dataframe is named df\nX = train.drop(columns='Rings', axis=1)\ny = train['Rings']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    ü§ùCORRELATIONü§ù\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://www.researchgate.net/publication/331676828/figure/tbl2/AS:735607286415366@1552393852839/Summary-of-Pearson-correlation-coefficient-r-strengths-Note-This-statistics-table-is.png)","metadata":{}},{"cell_type":"code","source":"merged_data = pd.concat([X, y.rename('Rings')], axis=1)\n\n# Calculate the correlation matrix\ncorrelation_matrix = merged_data.corr()\n\n# Plot the heatmap\nplt.figure(figsize=(10, 12))\nsns.heatmap(correlation_matrix[['Rings']], annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title('Correlation with respect to Rings')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_matrix = X.corr()\n\n# Set up the matplotlib figure with a larger size\nplt.figure(figsize=(18, 14))  # Increase the width and height as needed\n\n# Plot the correlation matrix as a heatmap with larger boxes\nheatmap = sns.heatmap(correlation_matrix, annot=True, cmap='Greens', fmt=\".2f\", linewidths=1, square=True)\n\n# Customize plot\nplt.title('Correlation Matrix', fontsize=20)  # Increase the font size of the title\nplt.xticks(fontsize=12)  # Increase the font size of x-axis labels\nplt.yticks(fontsize=12)  # Increase the font size of y-axis labels\n\n# Rotate the x-axis labels for better readability\nplt.xticks(rotation=45)\n\n# Adjust the aspect ratio to prevent distortion of cell shapes\nheatmap.set_aspect('equal')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    ‚ú®VARIANCE INFLUENCE FACTOR‚ú®\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://www.investopedia.com/thmb/gf0YzGsAInZdkoyB6jUGBkqLkZI=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/variance-inflation-factor.asp-Final-6cd8e4740c254821b0fa2ab057b5df88.jpg)","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif_data = pd.DataFrame()\nvif_data[\"Variable\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n# Sort the dataframe by VIF values in descending order\nvif_data = vif_data.sort_values(by='VIF', ascending=False)\n\n# Set a VIF threshold (e.g., VIF > 5)\nvif_threshold = 5\n\n# Identify variables with high VIF that exceed the threshold\nhigh_vif_variables = vif_data[vif_data[\"VIF\"] > vif_threshold]\n\n# Create a beautiful bar plot using Seaborn, highlighting high VIF variables\nplt.figure(figsize=(12, 12))\nsns.set(style=\"whitegrid\")\nplot = sns.barplot(x=\"VIF\", y=\"Variable\", data=vif_data, palette=\"viridis\")\n\n# Highlight high VIF variables\nfor bar in plot.patches:\n    if bar.get_width() > vif_threshold:\n        bar.set_color('coral')\n\nplt.xlabel('VIF')\nplt.ylabel('Variable')\nplt.title('Variance Inflation Factor (VIF) for Independent Variables')\nplt.xticks(fontsize=12)  # Increase font size of x-axis ticks\nplt.yticks(fontsize=12)  # Increase font size of y-axis ticks\nplt.grid(axis='x', linestyle='--', alpha=0.6, color='gray')  # Add grid lines\nplt.tight_layout()  # Adjust layout for better appearance\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display high VIF variables\nprint(\"Variables with high VIF (> {}):\".format(vif_threshold))\nhigh_vif_variables.style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n   üîçüîçHYPOTHESIS TESTINGüîçüîç\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://www.analyticssteps.com/backend/media/thumbnail/6735922/4237247_1626434645_HYPOTHESIS%20TESTINGArtboard%201.jpg)","metadata":{}},{"cell_type":"code","source":"target_columns = ['Rings']\n\n# Perform pairwise t-tests for each target column with all other columns\np_values = {}\nfor target in target_columns:\n    p_values[target] = {}  # Initialize a dictionary for storing p-values for this target\n    for column in train.columns:\n        if column != target:  # Exclude the target column itself\n            t_stat, p_value = ttest_ind(train[target], train[column])\n            p_values[target][column] = p_value\n\n# Convert the nested dictionary to a DataFrame for better visualization\np_values_df = pd.DataFrame(p_values)\n\n# Plotting the heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(p_values_df, cmap='viridis', annot=True, fmt=\".2f\")\nplt.title('Pairwise t-test p-values with respect to Rings')\nplt.xlabel('Columns')\nplt.ylabel('Target Columns')\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"P-values for Hypothesis Testing:\")\np_values_df.style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_columns = ['Rings']\n\n# Perform pairwise t-tests for each target column with all other columns\nsignificant_columns = {}\nfor target in target_columns:\n    p_values = {}  # Initialize a dictionary for storing p-values for this target\n    for column in train.columns:\n        if column != target:  # Exclude the target column itself\n            t_stat, p_value = ttest_ind(train[target], train[column])\n            p_values[column] = p_value\n    \n    # Filter columns based on p-value threshold (e.g., 0.05)\n    significant_columns[target] = [col for col, p_val in p_values.items() if p_val <= 0.05]\n\n# Display the number of significant columns for each target column\nfor target, cols in significant_columns.items():\n    print(f\"Number of significant columns for '{target}': {len(cols)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    üéáPCA (Principle Compenent Analysis)üéá\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/v2/resize:fit:1400/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg)","metadata":{}},{"cell_type":"code","source":"num_samples = len(X)\ncolors = np.random.rand(num_samples)\nX_array = X.values\n\n# Perform PCA\npca = PCA()\npca_result = pca.fit_transform(X_array)\n\n# Set a threshold for explained variance (e.g., 95%)\ncumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\nthreshold = 0.80\n\n# Find the number of components needed to exceed the threshold\nnum_components = np.argmax(cumulative_explained_variance >= threshold) + 1\n\n# Create a new DataFrame with principal components as columns\npca_columns = [f'PC{i}' for i in range(1, num_components + 1)]\npca_df = pd.DataFrame(data=pca_result[:, :num_components], columns=pca_columns)\n\n# Display the new DataFrame with principal components\nprint(\"DataFrame with Principal Components:\")\n\n# Visualize in 3D if there are at least 3 components\nif num_components >= 3:\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    scatter = ax.scatter(pca_df['PC1'], pca_df['PC2'], pca_df['PC3'], c=colors, cmap='viridis', alpha=0.5)\n    ax.set_title('PCA Result (3D) with Different Colors')\n    ax.set_xlabel('Principal Component 1')\n    ax.set_ylabel('Principal Component 2')\n    ax.set_zlabel('Principal Component 3')\n    colorbar = fig.colorbar(scatter, ax=ax, pad=0.1)\n    colorbar.set_label('Color Label')  # Update with appropriate label\n\n    plt.show()\nelse:\n    print(\"Number of components is less than 3, unable to visualize in 3D.\")\n    \npca_df.sample(10).style.background_gradient()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    Standardization \n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://dezyre.gumlet.io/images/blog/feature-scaling-in-machine-learning/Gaussian_Distribution.webp?w=376&dpr=2.6)","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming your dataframe is named df\nX_scale = train.drop(columns='Rings', axis=1)\ny_scale = train['Rings']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scale_S = ss.fit_transform(X_scale)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_scale_S","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    TRAIN - TEST - SPLIT \n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://media.licdn.com/dms/image/D4D12AQGDGMjsrzigDg/article-cover_image-shrink_600_2000/0/1674057624735?e=2147483647&v=beta&t=tVSJ36hIjz7sFWVEDRQuvBZxAtFu5JnlCiDpvIuDGI4)","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train2, X_test2, y_train2, y_test2 = train_test_split(X_scale_S, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    ‚ú®HYPERTUNING ‚ú®\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    ‚ú®XGBoost OPTUNA || XGBoost Classifier ‚ú®\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://analyticsindiamag.com/wp-content/uploads/2020/11/xgboost.png)","metadata":{}},{"cell_type":"code","source":"\"\"\"def objective(trial, X_train, y_train, X_test, y_test):\n    # Define parameters to be optimized for XGBRegressor\n    param = {\n        \"objective\": \"reg:squaredlogerror\",  # Use squared log error for regression\n        \"eval_metric\": \"rmsle\",  # Root Mean Squared Logarithmic Error for evaluation\n        \"verbosity\": 0,\n        \"booster\": \"gbtree\",\n        \"random_state\": 42,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.05),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 600),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.8, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 0.9),\n        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1, 10),\n    }\n\n    # Create an instance of XGBRegressor with the suggested parameters\n    xgb_regressor = XGBRegressor(**param)\n    \n    # Fit the regressor on the training data\n    xgb_regressor.fit(X_train, y_train)\n\n    # Predict on the test data\n    y_pred = xgb_regressor.predict(X_test)\n\n    # Compute RMSLE\n    rmsle = mean_squared_log_error(y_test, y_pred, squared=False)\n    \n    return rmsle\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_scale_S, y, test_size=0.2, random_state=42)  \n\n# Set up the sampler for Optuna optimization\nsampler = optuna.samplers.TPESampler(seed=42)  \n\n# Create a study object for Optuna optimization\nstudy = optuna.create_study(direction=\"minimize\", sampler=sampler)  # Use \"minimize\" for RMSLE\n\n# Run the optimization process\nstudy.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=100)\n\n# Get the best parameters after optimization\nbest_params = study.best_params\n\nprint('='*50)\nprint(best_params)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = {'learning_rate': 0.02708319027879099, 'n_estimators': 495, 'reg_alpha': 0.14219650343206225, \n               'reg_lambda': 0.5045620145662986, 'max_depth': 12, 'subsample': 0.9537603851451735,\n               'colsample_bytree': 0.7819555259366398, 'min_child_weight': 1.03921704772634}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XGBRegressor(**best_params)\nmodel.fit(X_scale_S, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance = model.feature_importances_\nfeature_names = X.columns\n\nsorted_indices = feature_importance.argsort()\nsorted_importance = feature_importance[sorted_indices]\nsorted_features = feature_names[sorted_indices]\n\nplt.figure(figsize=(10, 6))\ncolors = plt.cm.tab20c.colors[:len(sorted_features)]  \nplt.barh(sorted_features, sorted_importance, color=colors)\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('XGBoost Feature Importance')\nplt.gca().invert_yaxis() \nplt.tight_layout()  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    ‚ú®CATBoost OPTUNA || CATBoost Classifier ‚ú®\n</div>\n","metadata":{}},{"cell_type":"code","source":"\"\"\"from sklearn.metrics import mean_squared_error\ndef objective(trial, X_train, y_train, X_test, y_test):\n    # Define parameters to be optimized for CatBoostRegressor\n    param = {\n        \"objective\": \"RMSE\",  # Use Root Mean Squared Error (RMSE) as objective for regression\n        \"eval_metric\": \"RMSE\",  # Use RMSE for evaluation\n        \"random_state\": 42,\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 600),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.8, 1.0),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.3, 0.9),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n    }\n\n    # Create an instance of CatBoostRegressor with the suggested parameters\n    catboost_regressor = CatBoostRegressor(**param, verbose=0)\n    \n    # Fit the regressor on the training data\n    catboost_regressor.fit(X_train, y_train)\n\n    # Predict on the test data\n    y_pred = catboost_regressor.predict(X_test)\n\n    # Compute RMSE\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    \n    return rmse\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n\n# Set up the sampler for Optuna optimization\nsampler = optuna.samplers.TPESampler(seed=42)  \n\n# Create a study object for Optuna optimization\nstudy = optuna.create_study(direction=\"minimize\", sampler=sampler)  # Use \"minimize\" for RMSE\n\n# Run the optimization process\nstudy.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=100)\n\n# Get the best parameters after optimization\nbest_params = study.best_params\n\nprint('='*50)\nprint(best_params)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params_cat ={'learning_rate': 0.07855075323884125, 'n_estimators': 489, 'max_depth': 7,\n                  'subsample': 0.8569934338945397, 'colsample_bylevel': 0.8150591618201379,\n                  'reg_lambda': 0.4264547280178772}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cat = CatBoostRegressor(**best_params_cat)\nmodel_cat.fit(X_scale_S, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor, Pool, cv, MetricVisualizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pool = Pool(X_scale_S, y)\n\n# Get feature importance\nfeature_importance = model_cat.get_feature_importance(pool, type='PredictionValuesChange', prettified=True)\n\n# Sort feature importance values\nfeature_importance_sorted = feature_importance.sort_values(by='Importances', ascending=False)\n\n# Generate a color gradient for the bars\ncolors = sns.color_palette(\"viridis\", len(feature_importance_sorted))\n\n# Plot feature importance with customized colors and black border for bars\nplt.figure(figsize=(10, 6))\nbars = plt.barh(feature_importance_sorted['Feature Id'], feature_importance_sorted['Importances'], color=colors, edgecolor='black')  # Add edgecolor='black' for black borders\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('CatBoost Feature Importance')\nplt.gca().invert_yaxis()  # Invert y-axis to have the most important features at the top\nplt.tight_layout()  # Adjust layout to prevent cropping\n\n# Add color bar legend\nsm = plt.cm.ScalarMappable(cmap='viridis')\nsm.set_array(feature_importance_sorted['Importances'])\ncbar = plt.colorbar(sm, orientation='vertical')\ncbar.set_label('Importance Color Gradient')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cat.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cat.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    ‚ú®LGBM OPTUNA || LGBM Classifier ‚ú®\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://datadriven-rnd.com/wp-content/uploads/2021/10/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2021-10-04-21.47.12.png)","metadata":{}},{"cell_type":"code","source":"\"\"\"from sklearn.metrics import mean_squared_log_error\nimport lightgbm as lgb\n\n# Define the objective function for Optuna\ndef objective(trial):\n    # Define parameters to be optimized for LightGBM\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmsle\",\n        \"verbosity\": -1,\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 10, 1000),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 20),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n        \"random_state\": 42,\n    }\n\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_scale_S, y, test_size=0.2, random_state=42)\n\n    # Create a LightGBM Dataset for training and validation\n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_val, label=y_val)\n\n    # Train the LightGBM model\n    model = lgb.train(params, train_data, valid_sets=[train_data, val_data])\n\n    # Make predictions on the validation set\n    y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n\n    # Calculate RMSLE as the evaluation metric\n    rmsle = mean_squared_log_error(y_val, y_pred) ** 0.5\n\n    return rmsle\n\n# Set up the Optuna study\nstudy = optuna.create_study(direction=\"minimize\")  # Use \"minimize\" for RMSLE\n\n# Run the optimization process\nstudy.optimize(objective, n_trials=100)\n\n# Get the best parameters after optimization\nbest_params = study.best_params\n\nprint('='*50)\nprint(best_params)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print('Best Parameters:', study.best_params)\nbest_params_lgbm = {'boosting_type': 'gbdt', \n                    'learning_rate': 0.014659319896617274,\n                    'num_leaves': 302, 'max_depth': 7,\n                    'min_child_samples': 20,\n                    'subsample': 0.729207611815592,\n                    'colsample_bytree': 0.5372662767641898,\n                    'reg_alpha': 0.7606114726137472, \n                    'reg_lambda': 0.4319664094744963,\n                    'n_estimators': 766}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport lightgbm as lgb\n\n# Filter out LightGBM warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lgbm = LGBMRegressor(**best_params_lgbm)\nmodel_lgbm.fit(X,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance = model_lgbm.feature_importances_\n\nfeature_names = X.columns\n\nsorted_indices = feature_importance.argsort()\nsorted_importance = feature_importance[sorted_indices]\nsorted_features = feature_names[sorted_indices]\n\n# Plot feature importance\nplt.figure(figsize=(12, 8))\ncolors = plt.cm.Paired.colors[:len(sorted_features)]  \nplt.barh(sorted_features, sorted_importance, color=colors)\nplt.xlabel('Importance', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('LightGBM Feature Importance', fontsize=14)\nplt.gca().invert_yaxis() \n\nfor i, v in enumerate(sorted_importance):\n    plt.text(v + 0.02, i, f'{v:.2f}', color='black', va='center', fontsize=10)\n\nplt.tight_layout()  \nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#FFA500; padding: 20px; border-radius: 15px; font-size: 24px; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    <strong>üî• Neural Networks üî•</strong>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"![](https://www.unite.ai/wp-content/uploads/2023/05/Featured-Blog-Image-Liquid-Neural-Networks-Definition-Applications-and-Challenges.jpg)","metadata":{}},{"cell_type":"code","source":"train_nn.drop(columns=['id'], inplace=True)\ntest_nn.drop(columns=['id'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_nn['Sex_encoded'] = label_encoder.fit_transform(train_nn['Sex'])\ntest_nn['Sex_encodd']  = label_encoder.fit_transform(test_nn['Sex'])\ntrain_nn.drop(columns=['Sex'], inplace=True)\ntest_nn.drop(columns=['Sex'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_new = train_nn['Rings'] \nx_new = train_nn.drop(['Rings'],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_ann, X_test_ann, y_train_ann, y_test_ann = train_test_split(x_new, y_new, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n   üî• ANN Model - 1 üî•\n</div>\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.metrics import MeanSquaredError, MeanSquaredLogarithmicError\nfrom kerastuner import HyperModel\n\nclass MyHyperModel(HyperModel):\n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n\n    def build(self, hp):\n        model_ann = keras.Sequential()\n\n        # Number of dense layers\n        num_of_dense_layer = hp.Int('num_of_dense_layer', min_value=1, max_value=5, step=1)\n        model_ann.add(keras.layers.Input(shape=self.input_shape))\n        # Add dense layers\n        for i in range(num_of_dense_layer):\n            units = hp.Int('units_' + str(i), min_value=16, max_value=512, step=16)\n            dropout_rate = hp.Choice('dropout_rate_' + str(i), values=[0.1, 0.2, 0.3, 0.4, 0.5])\n\n            model_ann.add(keras.layers.Dense(units=units,\n                                              activation='relu',\n                                              kernel_initializer=keras.initializers.he_normal,\n                                              kernel_regularizer=keras.regularizers.l2(0.001)))\n            model_ann.add(keras.layers.BatchNormalization())\n            model_ann.add(keras.layers.Dropout(rate=dropout_rate))\n\n        # Output layer\n        model_ann.add(keras.layers.Dense(1, activation='linear'))  \n\n        # Compile the model\n        model_ann.compile(optimizer=keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n                          loss='mean_squared_logarithmic_error',  # Use mean squared logarithmic error for regression\n                          metrics=[MeanSquaredError(name=\"mean_squared_error\"),\n                                   MeanSquaredLogarithmicError(name=\"mean_squared_logarithmic_error\")]) \n\n        return model_ann\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\ninput_shape = X_train_ann.shape[1:]\nhypermodel = MyHyperModel(input_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nobjective = \"val_mean_squared_logarithmic_error\"\ntuner = keras_tuner.RandomSearch(\n    hypermodel,\n    objective=objective,\n    max_trials=5,\n    directory='/kaggle/working/',\n    project_name='Abalone-Age-3'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [\n    keras.callbacks.EarlyStopping(patience=3),  \n    keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=2) \n]\n\ntuner.search(X_train_ann, y_train_ann, epochs=50, validation_data=(X_test_ann, y_test_ann), callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\n# Build the model with the best hyperparameters\nbest_model_ann = tuner.hypermodel.build(best_hps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_ann.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\n# Plot the model with custom appearance\nplot_model(\n    best_model_ann,\n    to_file='model_plot.png', \n    show_shapes=True, \n    show_layer_names=True,\n    expand_nested=True,  \n    rankdir='TB', \n    dpi=300  \n)\n","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = best_model_ann.fit(X_train_ann, y_train_ann, epochs=100, validation_data=(X_test_ann, y_test_ann), callbacks=callbacks)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define the number of epochs\nepochs = range(1, len(history.history['loss']) + 1)\n\n# Create subplots\nfig, axs = plt.subplots(3, 1, figsize=(12, 18))\n\n# Plot training and validation loss\nsns.lineplot(x=epochs, y=history.history['loss'], label='Training Loss', marker='o', ax=axs[0], color='blue')\nsns.lineplot(x=epochs, y=history.history['val_loss'], label='Validation Loss', marker='s', ax=axs[0], color='orange')\naxs[0].set_title('Training and Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\n\n# Plot training and validation mean squared error (MSE)\nsns.lineplot(x=epochs, y=history.history['mean_squared_error'], label='Training MSE', marker='o', ax=axs[1], color='green')\nsns.lineplot(x=epochs, y=history.history['val_mean_squared_error'], label='Validation MSE', marker='s', ax=axs[1], color='red')\naxs[1].set_title('Training and Validation Mean Squared Error (MSE)')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('MSE')\naxs[1].legend()\n\n# Plot training and validation mean squared logarithmic error (MSLE)\nsns.lineplot(x=epochs, y=history.history['mean_squared_logarithmic_error'], label='Training MSLE', marker='o', ax=axs[2], color='purple')\nsns.lineplot(x=epochs, y=history.history['val_mean_squared_logarithmic_error'], label='Validation MSLE', marker='s', ax=axs[2], color='brown')\naxs[2].set_title('Training and Validation Mean Squared Logarithmic Error (MSLE)')\naxs[2].set_xlabel('Epochs')\naxs[2].set_ylabel('MSLE')\naxs[2].legend()\n\n# Set background color\nfig.patch.set_facecolor('lightgrey')\n\n# Add grid\nfor ax in axs:\n    ax.grid(True, linestyle='--', linewidth=0.5, color='black', alpha=0.5)\n\n# Add horizontal lines for better readability\nfor ax in axs:\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n    ax.axhline(0, color='black', linewidth=0.5)\n    ax.axvline(0, color='black', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on test data\ntest_loss, test_mse, test_msle = best_model_ann.evaluate(X_test_ann, y_test_ann)\n\n# Print the evaluation metrics\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Mean Squared Error (MSE):\", test_mse)\nprint(\"Test Mean Squared Logarithmic Error (MSLE):\", test_msle)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n   üî• ANN Model - 2 üî•\n</div>\n","metadata":{}},{"cell_type":"code","source":"class MyHyperModel1(HyperModel):\n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n\n    def build(self, hp):\n        model_ann_2 = keras.Sequential()\n\n        # Number of dense layers\n        num_of_dense_layer = hp.Int('num_of_dense_layer', min_value=1, max_value=5, step=1)\n        model_ann_2.add(keras.layers.Input(shape=self.input_shape))\n        \n        # Add dense layers\n        for i in range(num_of_dense_layer):\n            units = hp.Int('units_' + str(i), min_value=16, max_value=512, step=16)\n            dropout_rate = hp.Choice('dropout_rate_' + str(i), values=[0.1, 0.2, 0.3, 0.4, 0.5])\n            \n            # Add dense layer\n            model_ann_2.add(keras.layers.Dense(units=units,\n                                              activation='relu',\n                                              kernel_initializer=keras.initializers.he_normal,\n                                              kernel_regularizer=keras.regularizers.l2(0.001)))\n            # Add batch normalization\n            model_ann_2.add(keras.layers.BatchNormalization())\n            # Add dropout\n            model_ann_2.add(keras.layers.Dropout(rate=dropout_rate))\n\n        # Output layer\n        model_ann_2.add(keras.layers.Dense(1, activation='linear'))  \n\n        # Compile the model\n        model_ann_2.compile(optimizer=keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n                          loss='mean_squared_logarithmic_error',  # Use mean squared logarithmic error for regression\n                          metrics=[MeanSquaredError(name=\"mean_squared_error\"),\n                                   MeanSquaredLogarithmicError(name=\"mean_squared_logarithmic_error\")]) \n\n        return model_ann_2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape_2 = X_train2.shape[1:]\nhypermodel2 = MyHyperModel1(input_shape_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"objective = \"val_mean_squared_logarithmic_error\"\ntuner_1 = keras_tuner.RandomSearch(\n    hypermodel2,\n    objective=objective,\n    max_trials=5,\n    directory='/kaggle/working/',\n    project_name='Abalone-Age-4'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10),\n    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n]\ntuner_1.search(X_train2, y_train2, epochs=50, validation_data=(X_test2, y_test2), callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best hyperparameters\nbest_hps=tuner_1.get_best_hyperparameters(num_trials=1)[0]\n\n# Build the model with the best hyperparameters\nbest_model_ann2 = tuner_1.hypermodel.build(best_hps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_ann2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(\n    best_model_ann2,\n    to_file='model_plot_2.png', \n    show_shapes=True, \n    show_layer_names=True,\n    expand_nested=True,  \n    rankdir='TB', \n    dpi=300  \n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history2 = best_model_ann2.fit(X_train2, y_train2, epochs=100, validation_data=(X_test2, y_test2), callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(history2.history['loss']) + 1)\n\n# Create subplots\nfig, axs = plt.subplots(3, 1, figsize=(12, 18))\n\n# Plot training and validation loss\nsns.lineplot(x=epochs, y=history2.history['loss'], label='Training Loss', marker='o', ax=axs[0], color='blue')\nsns.lineplot(x=epochs, y=history2.history['val_loss'], label='Validation Loss', marker='s', ax=axs[0], color='orange')\naxs[0].set_title('Training and Validation Loss')\naxs[0].set_xlabel('Epochs')\naxs[0].set_ylabel('Loss')\naxs[0].legend()\n\n# Plot training and validation mean squared error (MSE)\nsns.lineplot(x=epochs, y=history2.history['mean_squared_error'], label='Training MSE', marker='o', ax=axs[1], color='green')\nsns.lineplot(x=epochs, y=history2.history['val_mean_squared_error'], label='Validation MSE', marker='s', ax=axs[1], color='red')\naxs[1].set_title('Training and Validation Mean Squared Error (MSE)')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('MSE')\naxs[1].legend()\n\n# Plot training and validation mean squared logarithmic error (MSLE)\nsns.lineplot(x=epochs, y=history2.history['mean_squared_logarithmic_error'], label='Training MSLE', marker='o', ax=axs[2], color='purple')\nsns.lineplot(x=epochs, y=history2.history['val_mean_squared_logarithmic_error'], label='Validation MSLE', marker='s', ax=axs[2], color='brown')\naxs[2].set_title('Training and Validation Mean Squared Logarithmic Error (MSLE)')\naxs[2].set_xlabel('Epochs')\naxs[2].set_ylabel('MSLE')\naxs[2].legend()\n\n# Set background color\nfig.patch.set_facecolor('yellow')\n\n# Add grid\nfor ax in axs:\n    ax.grid(True, linestyle='--', linewidth=0.5, color='black', alpha=0.5)\n\n# Add horizontal lines for better readability\nfor ax in axs:\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n    ax.axhline(0, color='black', linewidth=0.5)\n    ax.axvline(0, color='black', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n   üî• Prediction üî•\n</div>\n","metadata":{}},{"cell_type":"code","source":"test_pred = test.drop(columns='id',axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_xg = model.predict(test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_xg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_catboost = model_cat.predict(test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_catboost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_lgbm = model_lgbm.predict(test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_lgbm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_ann = best_model_ann.predict(test_nn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_ann","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_ann_2 = best_model_ann2.predict(test_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_ann_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submitted_csv = pd.read_csv('/kaggle/input/ann-aubmission/ANN_Submission_3.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_ann_3 = submitted_csv['Rings']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_ann_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#5642C5; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n    ‚ö°Prediction CSV File Submission‚ö°\n</div>\n","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/playground-series-s4e4/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['Rings'] = predictions_xg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"XGBoost_Submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['Rings'] = prediction_catboost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"CatBoost_Submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['Rings'] = prediction_lgbm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"LGMB_Submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['Rings'] = prediction_ann","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"ANN_Submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['Rings'] = prediction_ann_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"ANN_Submission2.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['Rings'] = prediction_ann_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"ANN_Submission3.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Image](https://static.fontget.com/t/h/thank-you/preview@2x.png)","metadata":{}}]}